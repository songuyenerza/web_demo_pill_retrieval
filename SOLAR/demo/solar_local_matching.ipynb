{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SOLAR: Second-Order Loss and Attention for Image Retrieval\n",
    "\n",
    "## Image matching visulisation of `solar_local` with `opencv`\n",
    "\n",
    "Below we show you a quick example of image matching performance, and compare with the baseline `SOSNet` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "%matplotlib notebook\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "torch.no_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialise SOSNet and SOLAR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First download [SOSNet](https://github.com/scape-research/SOSNet) weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://github.com/scape-research/SOSNet/raw/master/sosnet-weights/sosnet-32x32-liberty.pth -O /media/anlab/0e731fe3-5959-4d40-8958-e9f6296b38cb/home/anlab/songuyen/SOLAR/solar_local/weights/sosnet-32x32-liberty.pth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now load the networks and send them to cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0,'..')\n",
    "\n",
    "from solar_local.models.model import SOLAR_LOCAL, SOSNet32x32\n",
    "from solar_local.utils import describe_opencv\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# load the baseline SOSNet\n",
    "sosnet = SOSNet32x32()\n",
    "sosnet.load_state_dict(torch.load('/media/anlab/0e731fe3-5959-4d40-8958-e9f6296b38cb/home/anlab/songuyen/SOLAR/solar_local/weights/sosnet-32x32-liberty.pth'))\n",
    "sosnet = sosnet.to(device).eval()\n",
    "\n",
    "# load SOLAR\n",
    "solar_local = SOLAR_LOCAL(soa=True, soa_layers='345')\n",
    "solar_local.load_state_dict(torch.load('/media/anlab/0e731fe3-5959-4d40-8958-e9f6296b38cb/home/anlab/songuyen/SOLAR/solar_local/weights/local-solar-345-liberty.pth'))\n",
    "solar_local = solar_local.to(device).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(solar_local)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read images, detect keypoints and describe using SOSNet & SOLAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the images and detect BRISK keypoints using openCV\n",
    "img1 = cv2.imread('/media/anlab/0e731fe3-5959-4d40-8958-e9f6296b38cb/home/anlab/songuyen/SOLAR/demo/22.jpg',0)\n",
    "img1 = cv2.resize(img1, (600, 600))\n",
    "img2 = cv2.imread('/media/anlab/0e731fe3-5959-4d40-8958-e9f6296b38cb/home/anlab/songuyen/SOLAR/demo/22_23_24.jpg',0)\n",
    "img2 = cv2.resize(img2, (600, 600))\n",
    "\n",
    "brisk = cv2.BRISK_create(100)\n",
    "kp1 = brisk.detect(img1, None)\n",
    "kp2 = brisk.detect(img2, None)\n",
    "\n",
    "# We use the tfeat_utils methods that rectify patches around openCV keypoints and \n",
    "sosnet_desc_1 = describe_opencv(sosnet, img1, kp1, patch_size=32, mag_factor=3)\n",
    "sosnet_desc_2 = describe_opencv(sosnet, img2, kp2, patch_size=32, mag_factor=3)\n",
    "\n",
    "solar_desc_1 = describe_opencv(solar_local, img1, kp1, patch_size=32, mag_factor=3)\n",
    "solar_desc_2 = describe_opencv(solar_local, img2, kp2, patch_size=32, mag_factor=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "solar_desc_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sco = np.dot(solar_desc_1, solar_desc_2.T)\n",
    "sco\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brute-force matching with `opencv` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bf = cv2.BFMatcher(cv2.NORM_L2)\n",
    "matches_sosnet = bf.knnMatch(sosnet_desc_1, sosnet_desc_2, k=2)\n",
    "matches_solar = bf.knnMatch(solar_desc_1, solar_desc_2, k=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we use the same ratio for both descriptors (0.8, same as that from [here](https://github.com/scape-research/SOSNet/blob/master/SOSNet-demo.ipynb)) in the ratio test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply SIFT's ratio test, notice that 0.8 may not be the best ratio for SOSNet\n",
    "good_sosnet = []\n",
    "good_solar = []\n",
    "ratio = 0.8\n",
    "\n",
    "for m,n in matches_sosnet:\n",
    "    if m.distance < ratio*n.distance:\n",
    "        good_sosnet.append([m])\n",
    "        \n",
    "for m,n in matches_solar:\n",
    "    if m.distance < ratio*n.distance:\n",
    "        good_solar.append([m])\n",
    "       \n",
    "print('Number of matches: SOSNet {} | SOLAR {}'.format(len(good_sosnet), len(good_solar)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_solar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that SOLAR gives 15% more matches in this simple example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualising\n",
    "\n",
    "Now we plot the matches between the two images, for both descriptors, and save them as images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sosnet_img_matches = cv2.drawMatchesKnn(img1, kp1, img2, kp2, good_sosnet, 0, flags=2)\n",
    "solar_img_matches = cv2.drawMatchesKnn(img1, kp1, img2, kp2, good_solar, 0, flags=2)\n",
    "\n",
    "# save images\n",
    "cv2.imwrite(\"sosnet-matches.png\", sosnet_img_matches)\n",
    "cv2.imwrite(\"solar-matches.png\", solar_img_matches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also see the results with `matplotlib` here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(dpi=200)\n",
    "ax_sosnet = fig.add_subplot(211)\n",
    "ax_solar = fig.add_subplot(212)\n",
    "\n",
    "for ax in [ax_sosnet, ax_solar]:\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "\n",
    "# ax_sosnet.imshow(sosnet_img_matches)\n",
    "# ax_sosnet.set_title(\"SOSNET: Num Matches={}\".format(len(good_sosnet)))\n",
    "ax_solar.imshow(solar_img_matches)\n",
    "ax_solar.set_title(\"SOLAR: Num Matches={}\".format(len(good_solar)))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd /media/anlab/0e731fe3-5959-4d40-8958-e9f6296b38cb/home/anlab/songuyen/data_pill_1912/SOLAR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you look closely, SOLAR actually avoids an obvious wrong match that is present in SOSNet's example (top right of the left image matched to the right-end of the building in the right image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [-i IMAGE]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: --ip=127.0.0.1 --stdin=9003 --control=9001 --hb=9000 --Session.signature_scheme=\"hmac-sha256\" --Session.key=b\"3f119f99-7f0f-4a75-a418-9bee49352c4d\" --shell=9002 --transport=\"tcp\" --iopub=9004 --f=/tmp/tmp-3319rBQhB2FdQLof.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import os\n",
    "import time\n",
    "import pickle\n",
    "import pdb\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "\n",
    "import cv2\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from PIL import Image\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from solar_global.networks.imageretrievalnet import init_network\n",
    "from solar_global.datasets.datahelpers import default_loader \n",
    "from solar_global.utils.networks import load_network\n",
    "from solar_global.utils.plots import draw_soa_map\n",
    "\n",
    "\n",
    "MODEL = 'resnet101-solar-best.pth'\n",
    "IMSIZE = 1024\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"-i\", \"--image\", default=\"assets/グループ1①_1_3.png\", help=\"Path to the image\")\n",
    "args = parser.parse_args()\n",
    "\n",
    "def nothing(x):\n",
    "    pass\n",
    "\n",
    "\n",
    "# initialize the list of reference points and boolean indicating\n",
    "# whether cropping is being performed or not\n",
    "refPt = []\n",
    "cropping = False\n",
    " \n",
    "\n",
    "def click_and_crop(event, x, y, flags, param):\n",
    "\t# grab references to the global variables\n",
    "\tglobal refPt, cropping\n",
    " \n",
    "\t# if the left mouse button was clicked, record the starting\n",
    "\t# (x, y) coordinates and indicate that cropping is being\n",
    "\t# performed\n",
    "\tif event == cv2.EVENT_LBUTTONDOWN:\n",
    "\t\trefPt = [(x, y)]\n",
    "\t\tcropping = True\n",
    " \n",
    "\t# check to see if the left mouse button was released\n",
    "\telif event == cv2.EVENT_LBUTTONUP:\n",
    "\t\t# record the ending (x, y) coordinates and indicate that\n",
    "\t\t# the cropping operation is finished\n",
    "\t\trefPt.append((x, y))\n",
    "\t\tcropping = False\n",
    " \n",
    "\t\t# draw a rectangle around the region of interest\n",
    "\t\tcv2.rectangle(image, refPt[0], refPt[1], (0, 255, 0), 2)\n",
    "\t\tcv2.imshow(\"image\", image)\n",
    "\n",
    "\n",
    "# loading network\n",
    "net = load_network(network_name=MODEL)\n",
    "\n",
    "print(\">>>> loaded network: \")\n",
    "print(net.meta_repr())\n",
    "\n",
    "# moving network to gpu and eval mode\n",
    "net.cuda() \n",
    "net.eval() \n",
    "\n",
    "# load the image, clone it, and setup the mouse callback function\n",
    "image = cv2.imread(args.image)\n",
    "h, w = image.shape[0], image.shape[1]\n",
    "if (h <= w):\n",
    "    resize = (int(w * IMSIZE/h), IMSIZE)\n",
    "else:\n",
    "    resize = (IMSIZE, int(h * IMSIZE/w))\n",
    "image = cv2.resize(image, resize)\n",
    "clone = image.copy()\n",
    "# cv2.namedWindow(\"image\")\n",
    "cv2.setMouseCallback(\"image\", click_and_crop)\n",
    "\n",
    "\n",
    "while True: \n",
    "    # display the image and wait for a keypress\n",
    "    cv2.imshow(\"image\", image)\n",
    "    key = cv2.waitKey(20) #& 0xFF\n",
    " \n",
    "\t# if the 'r' key is pressed, reset the cropping region\n",
    "    if key == ord(\"r\"):\n",
    "        image = clone.copy()\n",
    "\t# if the 'c' key is pressed, break from the loop       \n",
    "    elif key == ord(\"q\"):\n",
    "        print(\"Exit\")\n",
    "        cv2.destroyAllWindows\n",
    "        break\n",
    "    if cv2.getWindowProperty('image', cv2.WND_PROP_VISIBLE) < 1:\n",
    "        break\n",
    " \n",
    "    # if there are two reference points, then crop the region of interest\n",
    "    # from the image and display it\n",
    "    if len(refPt) == 2:\n",
    "        # display soa\n",
    "        soa = draw_soa_map(default_loader(args.image), net, refPt)\n",
    "        cv2.imshow(\"Second order attention\", soa)\n",
    "        cv2.waitKey(20)\n",
    "\n",
    "# close all open windows\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --ignore-installed --upgrade jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ipykernel\n",
    "!python -m ipykernel install\n",
    "\n",
    "!conda install notebook ipykernel\n",
    "!ipython kernelspec install-self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
